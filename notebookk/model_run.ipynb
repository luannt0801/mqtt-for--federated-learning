{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import json\n",
    "import string\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "\n",
    "from tldextract import extract\n",
    "import math\n",
    "from tqdm import tqdm\n",
    " \n",
    "import logging\n",
    "import psutil\n",
    "from datetime import datetime\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_DGA =''\n",
    "max_features = 101\n",
    "embed_size = 64\n",
    "hidden_size = 64\n",
    "n_layers = 1\n",
    "\n",
    "maxlen = 127\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2ix = {x:idx+1 for idx, x in enumerate([c for c in string.printable])}\n",
    "ix2char = {ix:char for char, ix in char2ix.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(encoded_domains, maxlen):\n",
    "    domains = []\n",
    "    for domain in encoded_domains:\n",
    "        if len(domain) >= maxlen:\n",
    "            domains.append(domain[:maxlen])\n",
    "        else:\n",
    "            domains.append([0]*(maxlen-len(domain))+domain)\n",
    "    return np.asarray(domains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(df):\n",
    "    domains = df['domain'].to_numpy()\n",
    "    labels = df['label'].to_numpy()\n",
    "\n",
    "    char2ix = {x:idx+1 for idx, x in enumerate([c for c in string.printable])}\n",
    "    ix2char = {ix:char for char, ix in char2ix.items()}\n",
    "\n",
    "    # Convert characters to int and pad\n",
    "    encoded_domains = [[char2ix[y] for y in x] for x in domains]\n",
    "    encoded_labels = [0 if x == 0 else 1 for x in labels]\n",
    "    encoded_labels = np.asarray([label for idx, label in enumerate(encoded_labels) if len(encoded_domains[idx]) > 1])\n",
    "    encoded_domains = [domain for domain in encoded_domains if len(domain) > 1]\n",
    "\n",
    "    assert len(encoded_domains) == len(encoded_labels)\n",
    "\n",
    "    padded_domains = pad_sequences(encoded_domains, maxlen)\n",
    "    trainset = TensorDataset(torch.tensor(padded_domains, dtype=torch.long), torch.Tensor(encoded_labels))\n",
    "    trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    return trainloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision(x):\n",
    "    return x >= 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def domain2tensor(domains):\n",
    "    encoded_domains = [[char2ix[y] for y in domain] for domain in domains]\n",
    "    padded_domains = pad_sequences(encoded_domains, maxlen)\n",
    "    tensor_domains = torch.LongTensor(padded_domains)\n",
    "    return tensor_domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_state_dict(model, path):\n",
    "    # print(model.state_dict().items())\n",
    "    with open(path, 'w') as fp:\n",
    "        json.dump(fp=fp, obj={k:v.cpu().numpy().tolist() for k,v in model.state_dict().items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_state_dict(model, path):\n",
    "    # Need to initialize a new similar model and then apply loaded state_dict\n",
    "    with open(path, 'r') as fp:\n",
    "        state_dict = json.load(fp=fp)\n",
    "        state_dict = {k:torch.tensor(np.array(v)).to(device=device) for k,v in state_dict.items()}\n",
    "        model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataframe(start_line, start_main_dga, start_benign, num_line_arr, count, alpha):\n",
    "    data_folder = 'data'\n",
    "    #global start_line\n",
    "    dga_types = [dga_type for dga_type in os.listdir(data_folder) if os.path.isdir(os.path.join(data_folder, dga_type))]\n",
    "    #print(dga_types)\n",
    "    my_df = pd.DataFrame(columns=['domain', 'type', 'label'])\n",
    "    for dga_type in dga_types:\n",
    "        if(dga_type == MAIN_DGA):\n",
    "            files = os.listdir(os.path.join(data_folder, dga_type))\n",
    "            for file in files:\n",
    "                with open(os.path.join(data_folder, dga_type, file), 'r') as fp:\n",
    "                    # end = (start_main_dga + percent_main_dga*total_data_dgas)\n",
    "                    domains_with_type = [[(line.strip()), dga_type, 1] for line in fp.readlines()[start_main_dga:start_main_dga + int(alpha * num_line_arr[count]*10)]]\n",
    "                    print(f\"Main DGA \\n {start_main_dga}:{start_main_dga + int(alpha * num_line_arr[count]*10)}\")\n",
    "                    appending_df = pd.DataFrame(domains_with_type, columns=['domain', 'type', 'label'])\n",
    "                    my_df = pd.concat([my_df, appending_df], ignore_index=True)\n",
    "            \n",
    "            # print(\"Main DGA\")\n",
    "            # print(my_df['label'].value_counts())\n",
    "        else:\n",
    "            files = os.listdir(os.path.join(data_folder, dga_type))\n",
    "            for file in files:\n",
    "                with open(os.path.join(data_folder, dga_type, file), 'r') as fp:\n",
    "                    # ko main dga\n",
    "                    # domains_with_type = [[(line.strip()), dga_type, 1] for line in fp.readlines()[start_line:(start_line + (num_line_arr[count]))]]\n",
    "                    # co main dga\n",
    "                    domains_with_type = [[(line.strip()), dga_type, 1] for line in fp.readlines()[start_line:(start_line + int((((1-alpha)*10/9)*num_line_arr[count])))]]\n",
    "                    print(f\"9 dga \\n {start_line}:{(start_line + int((((1-alpha)*10/9)*num_line_arr[count])))}\")\n",
    "                    appending_df = pd.DataFrame(domains_with_type, columns=['domain', 'type', 'label'])\n",
    "                    my_df = pd.concat([my_df, appending_df], ignore_index=True)\n",
    "            # print(\"DGA - 9 cai\")\n",
    "            # print(my_df['label'].value_counts())\n",
    "            #print(f\"{dga_type}, {len(my_df)}\")\n",
    "            \n",
    "    with open(os.path.join(data_folder, 'benign.txt'), 'r') as fp:\n",
    "        print(int((num_line_arr[count]*10)))\n",
    "        domains_with_type = [[(line.strip()), 'benign', 0] for line in fp.readlines()[start_benign:(start_benign + int((num_line_arr[count]*10)))]]\n",
    "        print(\"end\", (start_benign + int((num_line_arr[count]*10))))\n",
    "        print(\"start\", start_benign)\n",
    "        appending_df = pd.DataFrame(domains_with_type, columns=['domain', 'type', 'label'])\n",
    "        my_df = pd.concat([my_df, appending_df], ignore_index=True)\n",
    "        \n",
    "    print(\"Total\")    \n",
    "    print(my_df['label'].value_counts())\n",
    "    return my_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling_noniid_data(df, fraction, num_labels, n):\n",
    "    label_data_list = []\n",
    "\n",
    "    num_samples_total = len(df)\n",
    "    num_samples_per_label = int(fraction * num_samples_total / num_labels)\n",
    "\n",
    "    # random_labels = random.sample(df['type'].unique(), n)\n",
    "    random_labels = random.sample(df['type'].unique().tolist(), n)\n",
    "\n",
    "    print(\"random label: \", random_labels)\n",
    "    # for label_name in df['type'].unique():\n",
    "    for label_name in random_labels:\n",
    "        print(\"Cac label: \",label_name)\n",
    "        label_df = df[df['type'] == label_name]\n",
    "        label_data = label_df.sample(n=num_samples_per_label, replace=True, random_state=0)\n",
    "        label_data_list.append(label_data)\n",
    "    final_df = pd.concat(label_data_list)\n",
    "\n",
    "def split_train_test_data(final_df):\n",
    "    train_test_df, val_df = train_test_split(final_df, test_size=0.1, shuffle=True) \n",
    "    #print(train_test_df)\n",
    "    # Pre-processing\n",
    "    domains = train_test_df['domain'].to_numpy()\n",
    "    labels = train_test_df['label'].to_numpy()\n",
    "\n",
    "    char2ix = {x:idx+1 for idx, x in enumerate([c for c in string.printable])}\n",
    "    ix2char = {ix:char for char, ix in char2ix.items()}\n",
    "\n",
    "    # Convert characters to int and pad\n",
    "    encoded_domains = [[char2ix[y] for y in x] for x in domains]\n",
    "    encoded_labels = [0 if x == 0 else 1 for x in labels]\n",
    "\n",
    "    #print(f\"Number of samples: {len(encoded_domains)}\")\n",
    "    #print(f\"One-hot dims: {len(char2ix) + 1}\")\n",
    "    encoded_labels = np.asarray([label for idx, label in enumerate(encoded_labels) if len(encoded_domains[idx]) > 1])\n",
    "    encoded_domains = [domain for domain in encoded_domains if len(domain) > 1]\n",
    "\n",
    "    assert len(encoded_domains) == len(encoded_labels)\n",
    "\n",
    "    padded_domains = pad_sequences(encoded_domains, maxlen)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(padded_domains, encoded_labels, test_size=0.10, shuffle=True)\n",
    "\n",
    "    trainset = TensorDataset(torch.tensor(X_train, dtype=torch.long), torch.Tensor(y_train))\n",
    "    trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "    testset = TensorDataset(torch.tensor(X_test, dtype=torch.long), torch.Tensor(y_test))\n",
    "    testloader = DataLoader(testset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "    return trainloader, testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, feat_size, embed_size, hidden_size, n_layers):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        \n",
    "        self.feat_size = feat_size\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(feat_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, n_layers, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        embedded_feats = self.embedding(x)\n",
    "        lstm_out, hidden = self.lstm(embedded_feats, hidden)\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_size)\n",
    "        fc_out = self.fc(lstm_out)\n",
    "        sigmoid_out = self.sigmoid(fc_out)\n",
    "        sigmoid_out = sigmoid_out.view(x.shape[0], -1)\n",
    "        sigmoid_last = sigmoid_out[:,-1]\n",
    "\n",
    "        return sigmoid_last, hidden\n",
    "    \n",
    "    def init_hidden(self, x):\n",
    "        weight = next(self.parameters()).data\n",
    "        h = (weight.new(self.n_layers, x.shape[0], self.hidden_size).zero_(),\n",
    "             weight.new(self.n_layers, x.shape[0], self.hidden_size).zero_())\n",
    "        return h\n",
    "    \n",
    "    def get_embeddings(self, x):\n",
    "        return self.embedding(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, trainloader, criterion, optimizer, epoch, batch_size):\n",
    "    model.train()\n",
    "    clip = 5\n",
    "    h = model.init_hidden(domain2tensor([\"0\"]*batch_size))\n",
    "    for inputs, labels in (tqdm(trainloader)):\n",
    "        \n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        h = tuple([each.data for each in h])\n",
    "\n",
    "        model.zero_grad()\n",
    "        output, h = model(inputs, h)\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        \n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, testloader, criterion, batch_size):\n",
    "    val_h = model.init_hidden(domain2tensor([\"0\"]*batch_size))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        eval_losses = []\n",
    "        total = 0\n",
    "        correct = 0\n",
    "      \n",
    "        for eval_inputs, eval_labels in tqdm(testloader):\n",
    "            \n",
    "            eval_inputs = eval_inputs.to(device)\n",
    "            eval_labels = eval_labels.to(device)\n",
    "            \n",
    "            val_h = tuple([x.data for x in val_h])\n",
    "            eval_output, val_h = model(eval_inputs, val_h)\n",
    "            \n",
    "            eval_prediction = decision(eval_output)\n",
    "            total += len(eval_prediction)\n",
    "            correct += sum(eval_prediction == eval_labels)\n",
    "               \n",
    "            eval_loss = criterion(eval_output.squeeze(), eval_labels.float())\n",
    "            eval_losses.append(eval_loss.item())\n",
    "\n",
    "    return np.mean(eval_losses), correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, testloader, batch_size):\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    h = model.init_hidden(batch_size)\n",
    "    model.eval()\n",
    "    for inp, lab in testloader:\n",
    "        h = tuple([each.data for each in h])\n",
    "        out, h = model(inp, h)\n",
    "        y_true.extend(lab)\n",
    "        preds = torch.round(out.squeeze())\n",
    "        y_pred.extend(preds)\n",
    "\n",
    "    print(roc_auc_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #my_df = save_dataframe()\n",
    "# #trainloader, testloader = split_train_test_data(my_df)\n",
    "# embed_size = 64\n",
    "# hidden_size = 64\n",
    "# net = LSTMModel(max_features, embed_size, hidden_size, n_layers)\n",
    "# torch.save(net.state_dict(), \"saved_model/LSTMModel.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_training_task(start_line, start_main_dga, start_benign, num_line_arr, count, alpha):\n",
    "    lr = 2e-5\n",
    "    epochs = 1\n",
    "    my_df = save_dataframe(start_line, start_main_dga, start_benign, num_line_arr, count, alpha)\n",
    "    trainloader, testloader = split_train_test_data(my_df)\n",
    "    \n",
    "    #cbi datta cho round say:\n",
    "    #start_line = start_line + num_line\n",
    "    #print(start_line)\n",
    "    #print(start_benign)\n",
    "    #print(start_main_dga)\n",
    "    model = LSTMModel(max_features, embed_size, hidden_size, n_layers).to(device)\n",
    "    model.load_state_dict(torch.load(\"newmode.pt\", map_location=device))\n",
    "    # model = BiLSTM(max_features, embed_size, hidden_size, n_layers).to(device)\n",
    "    criterion = nn.BCELoss(reduction='mean')\n",
    "    optimizer = optim.RMSprop(params=model.parameters(), lr=lr)\n",
    "\n",
    "    #logging.info(\"Using device: %s\", device)\n",
    "\n",
    "    time_start = time.time()\n",
    "    #logging.info(\"\\n Time start: %d\\n\", time_start)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        #logging.info(\"\\nEpoch: %d\\n\", epoch+1)\n",
    "        print(f\"\\nEpoch: {epoch+1}\")\n",
    "        train_loss = train(model=model, trainloader=trainloader, criterion=criterion, optimizer=optimizer, epoch=epoch, batch_size=batch_size)\n",
    "        eval_loss, accuracy = test(model=model, testloader=testloader, criterion=criterion, batch_size=batch_size)\n",
    "        print(\n",
    "            \"Epoch: {}/{}\".format(epoch+1, epochs),\n",
    "            \"Training Loss: {:.4f}\".format(train_loss.item()), \n",
    "            \"Eval Loss: {:.4f}\".format(eval_loss),\n",
    "            \"Accuracy: {:.4f}\".format(accuracy)\n",
    "        )\n",
    "        ram_usage = psutil.virtual_memory().percent\n",
    "        cpu_usage = psutil.cpu_percent()\n",
    "        #logging.info(\"\\nEpoch: {}/{} Training Loss: {:.4f} Eval Loss: {:.4f} Accuracy: {:.4f} Ram: {:.4f} CPU: {:.4f}\".format(\n",
    "         #               epoch + 1, epochs, train_loss.item(), eval_loss, accuracy, ram_usage, cpu_usage))\n",
    "\n",
    "    #print('Finished Training')\n",
    "    time_end = time.time()\n",
    "    #logging.info(\"\\n Time end: %d\\n\", time_end)\n",
    "    return model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregated_models(client_trainres_dict, n_round):\n",
    "    # Khởi tạo một OrderedDict để lưu trữ tổng của các tham số của mỗi layer\n",
    "    sum_state_dict = OrderedDict()\n",
    "\n",
    "    # Lặp qua các giá trị của dict chính và cộng giá trị của từng tham số vào sum_state_dict\n",
    "    for client_id, state_dict in client_trainres_dict.items():\n",
    "        for key, value in state_dict.items():\n",
    "            if key in sum_state_dict:\n",
    "                sum_state_dict[key] = sum_state_dict[key] + torch.tensor(value, dtype=torch.float32)\n",
    "            else:\n",
    "                sum_state_dict[key] = torch.tensor(value, dtype=torch.float32)\n",
    "\n",
    "    # Tính trung bình của các tham số\n",
    "    num_models = len(client_trainres_dict)\n",
    "    avg_state_dict = OrderedDict((key, value / num_models) for key, value in sum_state_dict.items())\n",
    "    torch.save(avg_state_dict, f'model_round_{n_round}.pt')\n",
    "    torch.save(avg_state_dict, \"saved_model/LSTMModel.pt\")\n",
    "    #delete parameter in client_trainres to start new round\n",
    "    client_trainres_dict.clear()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_training_task(client_id=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
